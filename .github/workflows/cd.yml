name: Continuous Deployment

on:
  workflow_run:
    workflows: ["Continuous Integration"]
    types:
      - completed
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'

jobs:
  check-ci-status:
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch'
    outputs:
      deploy: ${{ steps.check.outputs.deploy }}
    steps:
    - name: Check CI status
      id: check
      run: |
        if [[ "${{ github.event.workflow_run.conclusion }}" == "success" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          echo "deploy=true" >> $GITHUB_OUTPUT
        else
          echo "deploy=false" >> $GITHUB_OUTPUT
        fi

  build-and-push-docker:
    runs-on: ubuntu-latest
    needs: check-ci-status
    if: needs.check-ci-status.outputs.deploy == 'true'
    permissions:
      contents: read
      packages: write
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

    - name: Generate SBOM
      uses: anchore/sbom-action@v0
      with:
        image: ${{ steps.meta.outputs.tags }}
        format: spdx-json
        output-file: sbom.spdx.json

    - name: Upload SBOM
      uses: actions/upload-artifact@v4
      with:
        name: sbom
        path: sbom.spdx.json

  security-scan:
    runs-on: ubuntu-latest
    needs: build-and-push-docker
    steps:
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ needs.build-and-push-docker.outputs.image-tag }}
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  deploy-staging:
    runs-on: ubuntu-latest
    needs: [build-and-push-docker, security-scan]
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'staging')
    environment:
      name: staging
      url: https://staging.autonomous-sdlc.com
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region us-west-2 --name autonomous-sdlc-staging

    - name: Deploy to staging
      run: |
        # Update deployment with new image
        kubectl set image deployment/autonomous-sdlc-api \
          autonomous-sdlc-api=${{ needs.build-and-push-docker.outputs.image-tag }} \
          -n staging
        
        # Wait for rollout to complete
        kubectl rollout status deployment/autonomous-sdlc-api -n staging --timeout=600s
        
        # Update worker deployment
        kubectl set image deployment/autonomous-sdlc-worker \
          autonomous-sdlc-worker=${{ needs.build-and-push-docker.outputs.image-tag }} \
          -n staging
        
        kubectl rollout status deployment/autonomous-sdlc-worker -n staging --timeout=600s

    - name: Run deployment tests
      run: |
        # Wait for services to be ready
        kubectl wait --for=condition=ready pod -l app=autonomous-sdlc-api -n staging --timeout=300s
        
        # Run health checks
        STAGING_URL="https://staging.autonomous-sdlc.com"
        
        # API health check
        curl -f "${STAGING_URL}/health" || exit 1
        
        # A2A framework health check
        curl -f "${STAGING_URL}/api/v1/agents/health" || exit 1
        
        # Run smoke tests
        python tests/deployment/smoke_tests.py --environment=staging

    - name: Notify deployment success
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: '🚀 Staging deployment successful! Version: ${{ needs.build-and-push-docker.outputs.image-tag }}'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  deploy-production:
    runs-on: ubuntu-latest
    needs: [build-and-push-docker, security-scan, deploy-staging]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production'
    environment:
      name: production
      url: https://autonomous-sdlc.com
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
        aws-region: us-west-2

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region us-west-2 --name autonomous-sdlc-production

    - name: Create deployment backup
      run: |
        # Backup current deployment
        kubectl get deployment autonomous-sdlc-api -n production -o yaml > backup-api-deployment.yaml
        kubectl get deployment autonomous-sdlc-worker -n production -o yaml > backup-worker-deployment.yaml

    - name: Deploy to production with blue-green strategy
      run: |
        # Create green deployment
        sed 's/autonomous-sdlc-api/autonomous-sdlc-api-green/g' k8s/production/api-deployment.yaml > api-green-deployment.yaml
        sed 's/autonomous-sdlc-worker/autonomous-sdlc-worker-green/g' k8s/production/worker-deployment.yaml > worker-green-deployment.yaml
        
        # Update image in green deployments
        sed -i "s|image:.*|image: ${{ needs.build-and-push-docker.outputs.image-tag }}|g" api-green-deployment.yaml
        sed -i "s|image:.*|image: ${{ needs.build-and-push-docker.outputs.image-tag }}|g" worker-green-deployment.yaml
        
        # Deploy green version
        kubectl apply -f api-green-deployment.yaml -n production
        kubectl apply -f worker-green-deployment.yaml -n production
        
        # Wait for green deployment to be ready
        kubectl rollout status deployment/autonomous-sdlc-api-green -n production --timeout=600s
        kubectl rollout status deployment/autonomous-sdlc-worker-green -n production --timeout=600s

    - name: Run production readiness tests
      run: |
        # Test green deployment internally
        GREEN_SERVICE_IP=$(kubectl get service autonomous-sdlc-api-green -n production -o jsonpath='{.spec.clusterIP}')
        
        # Health checks on green deployment
        kubectl run test-pod --image=curlimages/curl --rm -i --restart=Never -n production -- \
          curl -f "http://${GREEN_SERVICE_IP}:8000/health"
        
        # Run comprehensive tests
        python tests/deployment/production_readiness_tests.py --target=green

    - name: Switch traffic to green deployment
      run: |
        # Update service selectors to point to green deployment
        kubectl patch service autonomous-sdlc-api -n production -p '{"spec":{"selector":{"app":"autonomous-sdlc-api-green"}}}'
        kubectl patch service autonomous-sdlc-worker -n production -p '{"spec":{"selector":{"app":"autonomous-sdlc-worker-green"}}}'
        
        # Wait a moment for traffic to switch
        sleep 30

    - name: Verify production deployment
      run: |
        # Final health checks
        PROD_URL="https://autonomous-sdlc.com"
        
        # API health check
        curl -f "${PROD_URL}/health" || exit 1
        
        # A2A framework health check
        curl -f "${PROD_URL}/api/v1/agents/health" || exit 1
        
        # Run production smoke tests
        python tests/deployment/smoke_tests.py --environment=production

    - name: Clean up old deployment
      run: |
        # Remove blue (old) deployment
        kubectl delete deployment autonomous-sdlc-api -n production --ignore-not-found=true
        kubectl delete deployment autonomous-sdlc-worker -n production --ignore-not-found=true
        
        # Rename green to current
        kubectl patch deployment autonomous-sdlc-api-green -n production -p '{"metadata":{"name":"autonomous-sdlc-api"}}'
        kubectl patch deployment autonomous-sdlc-worker-green -n production -p '{"metadata":{"name":"autonomous-sdlc-worker"}}'

    - name: Update deployment metadata
      run: |
        # Tag deployment with metadata
        kubectl annotate deployment autonomous-sdlc-api -n production \
          deployment.kubernetes.io/revision="$(date +%Y%m%d-%H%M%S)" \
          deployment.kubernetes.io/image="${{ needs.build-and-push-docker.outputs.image-tag }}" \
          deployment.kubernetes.io/commit="${{ github.sha }}"

    - name: Notify production deployment
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: '🎉 Production deployment successful! Version: ${{ needs.build-and-push-docker.outputs.image-tag }}'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  rollback-on-failure:
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]
    if: failure()
    steps:
    - name: Rollback staging on failure
      if: failure() && needs.deploy-staging.result == 'failure'
      run: |
        kubectl rollout undo deployment/autonomous-sdlc-api -n staging
        kubectl rollout undo deployment/autonomous-sdlc-worker -n staging

    - name: Rollback production on failure
      if: failure() && needs.deploy-production.result == 'failure'
      run: |
        # Restore from backup
        kubectl apply -f backup-api-deployment.yaml -n production
        kubectl apply -f backup-worker-deployment.yaml -n production
        
        # Wait for rollback to complete
        kubectl rollout status deployment/autonomous-sdlc-api -n production --timeout=300s
        kubectl rollout status deployment/autonomous-sdlc-worker -n production --timeout=300s

    - name: Notify rollback
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: '⚠️ Deployment failed and rollback initiated'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  deployment-summary:
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]
    if: always()
    steps:
    - name: Generate deployment summary
      run: |
        echo "## Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Image**: ${{ needs.build-and-push-docker.outputs.image-tag }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Staging**: ${{ needs.deploy-staging.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Production**: ${{ needs.deploy-production.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Timestamp**: $(date -u)" >> $GITHUB_STEP_SUMMARY